train: # Training parameters
  model: /model-weights/Llama-2-7b-hf/
  # datasets: # One or more datasets to be used for training, the csvs have to have the same columns
  #   - "../data/example_project_data/prepared_generated_data_for_example_project.csv" 
  #   - "../data/nhs_uk_full/prepared_generated_data_for_nhs_uk_qa.csv"
  #   - "../data/nhs_uk_full/prepared_generated_data_for_nhs_uk_conversations.csv"
  #   - "../data/medical_tasks_gpt4/prepared_generated_data_for_medical_tasks.csv"
  train_ds: /checkpoint/opt_test/original/clinical_llm/llama-2-7b-ft-mdpi-mtb-5-epochs-new-lr/datasets/mdpi_mtb_processed/train/
  test_ds: /checkpoint/opt_test/original/clinical_llm/llama-2-7b-ft-mdpi-mtb-5-epochs-new-lr/datasets/mdpi_mtb_processed/test/
  ignore_index: -100 # This will be added as label if we want to skip something
  max_seq_len: 1024 # Should match the models max seq len, or be smaller
  packing_type: 'partial' # one of 'partial', 'full' or 'none' - IMPORTANT, but experimental, Full/Partial will speedup the training drastically (2-3x)
  hf_training_arguments:
    checkpointing_enabled: True  # change
    gradient_accumulation_steps: 8 # Aim for a BS of 128, forumla is: n_dev * batch_size * acc_steps
    per_device_eval_batch_size: 2
    per_device_train_batch_size: 4
    learning_rate: 2.0e-5
    weight_decay: 0.1
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_epsilon: 1.0e-7
    num_processes: 4
    max_grad_norm: 1
    num_train_epochs: 5
    lr_scheduler_type: 'cosine'
    warmup_ratio: 0.05
    logging_strategy: 'steps'
    logging_steps: 500
    save_frequency: 0.25
    seed: 11
  wandb_config:
    project: "MedGPT"
    name: "Llama-2-7B-ft"