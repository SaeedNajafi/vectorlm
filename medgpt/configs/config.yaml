model: /model-weights/Llama-2-7b-hf/
# datasets:
#   - "/h/adilasif/OpenGPT/data/example_project_data/prepared_generated_data_for_example_project.csv" 
#   - "/h/adilasif/OpenGPT/data/nhs_uk_full/prepared_generated_data_for_nhs_uk_qa.csv"
#   - "/h/adilasif/OpenGPT/data/nhs_uk_full/prepared_generated_data_for_nhs_uk_conversations.csv"
#   - "/h/adilasif/OpenGPT/data/medical_tasks_gpt4/prepared_generated_data_for_medical_tasks.csv"

wandb_config:
  project: MedGPT
  name: Llama-2-7B-ft

train_parameters:
  max_seq_len: 1024 # Should be <= model's max sequence length
  checkpointing_enabled: True # If we should checkpoint model training
  num_processes: 4 # Total number of GPUs
  epochs: 1
  seed: 11

  # Memory
  use_mp: True
  use_activation_checkpointing: True
  use_flash_attention: True

  # Gradient norm clipping
  max_grad_norm: 1
  lr_scheduler_type: cosine # TODO: include plataeu
  warmup_ratio: 0.05
  gradient_accumulation_steps: 16 # Aim for a BS of 128, forumla is: n_dev * batch_size * acc_steps

  # Optimizer
  optimizer:
    lr: 2.0e-5
    weight_decay: 0.1
    betas: [0.9, 0.95]
    eps: 1.0e-5
  
  # Scheduler

  # Checkpointing
  checkpointing_enabled: False
  logging_steps: 100  # TODO: change to 500
  save_frequency: 0.25  # TODO: change to 0.25s

dataset:
  ignore_index: -100 # The index used to exclude a token from loss calculation
  save_proc_ds: False # Boolean to save processed dataset. Use once to cache
  dataset_format: hf # Or json
  dataset_type: pretrain # or sft
  data_fields: text
  eval_bs: 2 # Per device batch size
  train_bs: 2 # Per device batch size
  train_ds: /checkpoint/opt_test/original/clinical_llm/datasets/mdpi_mtb_processed/train/
  test_ds: /checkpoint/opt_test/original/clinical_llm/datasets/mdpi_mtb_processed/test/